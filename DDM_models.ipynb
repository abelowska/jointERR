{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DDM stan model fitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48a67958d0ac9973"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3389f6ef5f25776c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stan\n",
    "import nest_asyncio\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# enables multithreading in jupyter notebook\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da9d19f4990d66f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams['savefig.dpi'] = 300"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e06b9719d00ad538"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stan model code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a71f2b0753ad095"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DDM_code = \"\"\"\n",
    "functions {\n",
    "  /* Wiener diffusion log-PDF for a single response (adapted from brms 1.10.2)\n",
    "   * Arguments:\n",
    "   *   Y: acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "   *   boundary: boundary separation parameter > 0\n",
    "   *   ndt: non-decision time parameter > 0\n",
    "   *   bias: initial bias parameter in [0, 1]\n",
    "   *   drift: drift rate parameter\n",
    "   * Returns:\n",
    "   *   a scalar to be added to the log posterior\n",
    "   */\n",
    "   real diffusion_lpdf(real Y, real boundary,\n",
    "                              real ndt, real bias, real drift) {\n",
    "\n",
    "    if (Y >= 0) {\n",
    "        return wiener_lpdf( abs(Y) | boundary, ndt, bias, drift ); // change to abs()\n",
    "    } else {\n",
    "        return wiener_lpdf( abs(Y) | boundary, ndt, 1-bias, -drift ); // change to abs()\n",
    "    }\n",
    "\n",
    "   }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> N; // Number of trial-level observations\n",
    "    int<lower=1> n_conditions; // Number of conditions (congruent and incongruent)\n",
    "    int<lower=1> n_participants; // Number of participants\n",
    "\n",
    "    array[N] real y; // acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    array[N] int<lower=1> participant; // Participant index\n",
    "    array[N] int condition; // Condition index\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0, upper=0.3>[n_participants] participants_ter; // Participant-level Non-decision time\n",
    "    vector<lower=0, upper=3>[n_participants] participants_alpha; // Participant-level Boundary parameter (speed-accuracy tradeoff)\n",
    "    vector<lower=0, upper=1>[n_participants] participants_beta; // Participant-level Start point bias towards choice A\n",
    "    matrix[n_participants,n_conditions] participants_condition_delta; // Participant-level and condition-level drift rate to choice A\n",
    "}\n",
    "\n",
    "model {\n",
    "    // ##########\n",
    "    // Participant-level DDM parameter priors\n",
    "    // ##########\n",
    "    for (p in 1:n_participants) {\n",
    "\n",
    "        // Participant-level non-decision time\n",
    "        participants_ter[p] ~ normal(.05, .2) T[0, .3];\n",
    "\n",
    "        // Participant-level boundary parameter (speed-accuracy tradeoff)\n",
    "        participants_alpha[p] ~ normal(1., 1.) T[0, 3];\n",
    "\n",
    "        //Participant-level start point bias towards choice A\n",
    "        participants_beta[p] ~ normal(.5, .1) T[0, 1];\n",
    "\n",
    "        //Participant-level and condition-level drift rate\n",
    "        for (c in 1:n_conditions) {\n",
    "            participants_condition_delta[p,c] ~ normal(0., 1.); // Participant-level and condition-level drift rate\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Wiener likelihood\n",
    "    for (i in 1:N) {\n",
    "\n",
    "        target += diffusion_lpdf( y[i] | participants_alpha[participant[i]], participants_ter[participant[i]], participants_beta[participant[i]], participants_condition_delta[participant[i], condition[i]]);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8e6319485107bbc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DDM_delta_decomposed_code = \"\"\"\n",
    "functions {\n",
    "  /* Wiener diffusion log-PDF for a single response (adapted from brms 1.10.2)\n",
    "   * Arguments:\n",
    "   *   Y: acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "   *   boundary: boundary separation parameter > 0\n",
    "   *   ndt: non-decision time parameter > 0\n",
    "   *   bias: initial bias parameter in [0, 1]\n",
    "   *   drift: drift rate parameter\n",
    "   * Returns:\n",
    "   *   a scalar to be added to the log posterior\n",
    "   */\n",
    "   real diffusion_lpdf(real Y, real boundary,\n",
    "                              real ndt, real bias, real drift) {\n",
    "\n",
    "    if (Y >= 0) {\n",
    "        return wiener_lpdf( abs(Y) | boundary, ndt, bias, drift ); // change to abs()\n",
    "    } else {\n",
    "        return wiener_lpdf( abs(Y) | boundary, ndt, 1-bias, -drift ); // change to abs()\n",
    "    }\n",
    "\n",
    "   }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> N; // Number of trial-level observations\n",
    "    int<lower=1> n_conditions; // Number of conditions (congruent and incongruent)\n",
    "    int<lower=1> n_participants; // Number of participants\n",
    "\n",
    "    array[N] real y; // acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    array[N] int condition; // Contrast coded condition: -1 for erroneous and 1 for correct response respectively\n",
    "    array[N] int<lower=1> participant; // Participant index\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0, upper=0.3>[n_participants] participants_ter; // Participant-level Non-decision time\n",
    "    vector<lower=0, upper=3>[n_participants] participants_alpha; // Participant-level Boundary parameter (speed-accuracy tradeoff)\n",
    "    vector<lower=0, upper=1>[n_participants] participants_beta; // Participant-level Start point bias towards choice A\n",
    "    vector[n_participants] participants_delta; // Participant-level drift-rate\n",
    "    vector[n_participants] participants_delta_theta; // Per-participant condition-level drift-rate adjustment \n",
    "}\n",
    "\n",
    "model {\n",
    "    // ##########\n",
    "    // Participant-level DDM parameter priors\n",
    "    // ##########\n",
    "    for (p in 1:n_participants) {\n",
    "\n",
    "        // Participant-level non-decision time\n",
    "        participants_ter[p] ~ normal(.1, .2) T[0, .3];\n",
    "\n",
    "        // Participant-level boundary parameter (speed-accuracy tradeoff)\n",
    "        participants_alpha[p] ~ normal(1., 1.) T[0, 3];\n",
    "\n",
    "        //Participant-level start point bias towards choice A\n",
    "        participants_beta[p] ~ normal(.5, .2) T[0, 1];\n",
    "        \n",
    "        //Participant-level drift rate\n",
    "        participants_delta[p] ~ normal(0, 1);\n",
    "        \n",
    "        //Participant-level condition_adjustment\n",
    "        participants_delta_theta[p] ~ normal(0, 1);        \n",
    "\n",
    "    }\n",
    "\n",
    "    // Wiener likelihood\n",
    "    for (i in 1:N) {\n",
    "\n",
    "        target += diffusion_lpdf( y[i] | participants_alpha[participant[i]], participants_ter[participant[i]], participants_beta[participant[i]], participants_delta[participant[i]] + participants_delta_theta[participant[i]]*condition[i]);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "479c0f44ca910b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read and prepare data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51eba714e6303334"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('two_participants_test_set.csv').drop(columns='Unnamed: 0')\n",
    "\n",
    "# check dataframe\n",
    "display(df.isnull().any())\n",
    "display(df.head())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f294014ec1d9970"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove trials with RT < 100ms for model to converge (problem with non-decision time)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77399b0e069d8339"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_rts_truncated = df[df['rt'] > 0.1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb03f9f1174237b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare data for Stan"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc7a0be60da39e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = df_rts_truncated['y'].to_numpy()\n",
    "participant = df_rts_truncated['participant_index'].to_numpy()\n",
    "# condition_index = df_trun['condition_index'].to_numpy()\n",
    "condition = df_rts_truncated['condition'].to_numpy()\n",
    "n_participants = len(np.unique(participant))\n",
    "n_conditions = len(np.unique(condition))\n",
    "\n",
    "print(f\"Number of participants: {n_participants}\\nNumber of conditions: {n_conditions}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70748cd12b58a150"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"N\": len(y),\n",
    "    \"n_conditions\": n_conditions,\n",
    "    \"n_participants\": n_participants,\n",
    "    \"y\": y,\n",
    "    \"participant\": participant,\n",
    "    \"condition\": condition,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa331e315a36ed5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and fit the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f279e6c638c44b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_chains = 4\n",
    "warmup = 1000\n",
    "num_samples = 10000\n",
    "\n",
    "# todo - chains' init\n",
    "posterior = stan.build(DDM_delta_decomposed_code, data=data, random_seed=42)\n",
    "posterior\n",
    "# fit = posterior.sample(num_chains=num_chains, num_samples=num_samples, num_warmup = warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fit = posterior.sample(num_chains=num_chains, num_samples=num_samples, num_warmup = warmup, save_warmup=True)\n",
    "fit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e48a90e88d706496"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract samples and chains"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5eafb84fe80cf2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fit_df = fit.to_frame()\n",
    "\n",
    "# adds chain number to dataframe with draws_. See: link_to_pull_request\n",
    "chains = np.ones((num_samples, 1), int) * np.arange(num_chains)\n",
    "fit_df.insert(0, \"chain__\", chains.ravel())\n",
    "\n",
    "fit_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48e921eab284ca27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f92d5e2ac59b6f95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary of the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a080f3f5fe2795a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variables_to_track = list(posterior.constrained_param_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71b7d7d3243bcaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# overall summary\n",
    "fit_df[variables_to_track].describe().T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ea178d73017cc3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# summary by chain\n",
    "fit_df.groupby(['chain__'])[variables_to_track].describe().T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9624f4a15fb963e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Posterior and chains plots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0782e5142329116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,10))\n",
    "\n",
    "melted_df = pd.melt(fit_df, id_vars=list(filter(lambda x: x not in set(variables_to_track),fit_df.columns.to_list())), var_name='parameter_name', value_name='draws')\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    melted_df,\n",
    "    col=\"parameter_name\",\n",
    "    col_wrap=2,\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    aspect=2,\n",
    "    hue='chain__',\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.histplot,\n",
    "    x=\"draws\",\n",
    "    kde=True,\n",
    ")\n",
    "\n",
    "g.add_legend()\n",
    "# plt.savefig('parameters_posteriors.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    melted_df,\n",
    "    col=\"parameter_name\",\n",
    "    col_wrap=2,\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    aspect=2,\n",
    "    hue='chain__',\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=np.arange(0,num_samples),\n",
    "    y=\"draws\",\n",
    ")\n",
    "\n",
    "g.add_legend()\n",
    "# plt.savefig('chains.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a59596fb519d975d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Diagnostics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15fb5d61df99542d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adapted from https://github.com/mdnunez/pyhddmjags/tree/master\n",
    "def diagnostic(insamples):\n",
    "    \"\"\"\n",
    "    Returns two versions of Rhat (measure of convergence, less is better with an approximate\n",
    "    1.10 cutoff) and Neff, number of effective samples). Note that 'rhat' is more diagnostic than 'oldrhat' according to \n",
    "    Gelman et al. (2014).\n",
    "\n",
    "    Reference for preferred Rhat calculation (split chains) and number of effective sample calculation: \n",
    "        Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. & Rubin, D. B. (2014). \n",
    "        Bayesian data analysis (Third Edition). CRC Press:\n",
    "        Boca Raton, FL\n",
    "\n",
    "    Reference for original Rhat calculation:\n",
    "        Gelman, A., Carlin, J., Stern, H., & Rubin D., (2004).\n",
    "        Bayesian Data Analysis (Second Edition). Chapman & Hall/CRC:\n",
    "        Boca Raton, FL.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    insamples: dic\n",
    "        Sampled values of monitored variables as a dictionary where keys\n",
    "        are variable names and values are numpy arrays with shape:\n",
    "        (dim_1, dim_n, iterations, chains). dim_1, ..., dim_n describe the\n",
    "        shape of variable in JAGS model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        rhat, oldrhat, neff, posterior mean, and posterior std for each variable. Prints maximum Rhat and minimum Neff across all variables\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}  # Initialize dictionary\n",
    "    maxrhatsold = np.zeros((len(insamples.keys())), dtype=float)\n",
    "    maxrhatsnew = np.zeros((len(insamples.keys())), dtype=float)\n",
    "    minneff = np.ones((len(insamples.keys())), dtype=float)*np.inf\n",
    "    allkeys ={} # Initialize dictionary\n",
    "    keyindx = 0\n",
    "    for key in insamples.keys():\n",
    "        if key[0] != '_':\n",
    "            result[key] = {}\n",
    "\n",
    "            possamps = insamples[key]\n",
    "\n",
    "            # Number of chains\n",
    "            nchains = possamps.shape[-1]\n",
    "\n",
    "            # Number of samples per chain\n",
    "            nsamps = possamps.shape[-2]\n",
    "\n",
    "            # Number of variables per key\n",
    "            nvars = np.prod(possamps.shape[0:-2])\n",
    "\n",
    "            # Reshape data\n",
    "            allsamps = np.reshape(possamps, possamps.shape[:-2] + (nchains * nsamps,))\n",
    "\n",
    "            # Reshape data to preduce R_hatnew\n",
    "            possampsnew = np.empty(possamps.shape[:-2] + (int(nsamps/2), nchains * 2,))\n",
    "            newc=0\n",
    "            for c in range(nchains):\n",
    "                possampsnew[...,newc] = np.take(np.take(possamps,np.arange(0,int(nsamps/2)),axis=-2),c,axis=-1)\n",
    "                possampsnew[...,newc+1] = np.take(np.take(possamps,np.arange(int(nsamps/2),nsamps),axis=-2),c,axis=-1)\n",
    "                newc += 2\n",
    "\n",
    "            # Index of variables\n",
    "            varindx = np.arange(nvars).reshape(possamps.shape[0:-2])\n",
    "\n",
    "            # Reshape data\n",
    "            alldata = np.reshape(possamps, (nvars, nsamps, nchains))\n",
    "\n",
    "            # Mean of each chain for rhat\n",
    "            chainmeans = np.mean(possamps, axis=-2)\n",
    "            # Mean of each chain for rhatnew\n",
    "            chainmeansnew = np.mean(possampsnew, axis=-2)\n",
    "            # Global mean of each parameter for rhat\n",
    "            globalmean = np.mean(chainmeans, axis=-1)\n",
    "            globalmeannew = np.mean(chainmeansnew, axis=-1)\n",
    "            result[key]['mean'] = globalmean\n",
    "            result[key]['std'] = np.std(allsamps, axis=-1)\n",
    "            globalmeanext = np.expand_dims(\n",
    "                globalmean, axis=-1)  # Expand the last dimension\n",
    "            globalmeanext = np.repeat(\n",
    "                globalmeanext, nchains, axis=-1)  # For differencing\n",
    "            globalmeanextnew = np.expand_dims(\n",
    "                globalmeannew, axis=-1)  # Expand the last dimension\n",
    "            globalmeanextnew = np.repeat(\n",
    "                globalmeanextnew, nchains*2, axis=-1)  # For differencing\n",
    "            # Between-chain variance for rhat\n",
    "            between = np.sum(np.square(chainmeans - globalmeanext),\n",
    "                             axis=-1) * nsamps / (nchains - 1.)\n",
    "            # Mean of the variances of each chain for rhat\n",
    "            within = np.mean(np.var(possamps, axis=-2), axis=-1)\n",
    "            # Total estimated variance for rhat\n",
    "            totalestvar = (1. - (1. / nsamps)) * \\\n",
    "                          within + (1. / nsamps) * between\n",
    "            # Rhat (original Gelman-Rubin statistic)\n",
    "            temprhat = np.sqrt(totalestvar / within)\n",
    "            maxrhatsold[keyindx] = np.nanmax(temprhat) # Ignore NANs\n",
    "            allkeys[keyindx] = key\n",
    "            result[key]['oldrhat'] = temprhat\n",
    "            # Between-chain variance for rhatnew\n",
    "            betweennew = np.sum(np.square(chainmeansnew - globalmeanextnew),\n",
    "                                axis=-1) * (nsamps/2) / ((nchains*2) - 1.)\n",
    "            # Mean of the variances of each chain for rhatnew\n",
    "            withinnew = np.mean(np.var(possampsnew, axis=-2), axis=-1)\n",
    "            # Total estimated variance\n",
    "            totalestvarnew = (1. - (1. / (nsamps/2))) * \\\n",
    "                             withinnew + (1. / (nsamps/2)) * betweennew\n",
    "            # Rhatnew (Gelman-Rubin statistic from Gelman et al., 2013)\n",
    "            temprhatnew = np.sqrt(totalestvarnew / withinnew)\n",
    "            maxrhatsnew[keyindx] = np.nanmax(temprhatnew) # Ignore NANs\n",
    "            result[key]['rhat'] = temprhatnew\n",
    "            # Number of effective samples from Gelman et al. (2013) 286-288\n",
    "            neff = np.empty(possamps.shape[0:-2])\n",
    "            for v in range(0, nvars):\n",
    "                whereis = np.where(varindx == v)\n",
    "                rho_hat = []\n",
    "                rho_hat_even = 0\n",
    "                rho_hat_odd = 0\n",
    "                t = 2\n",
    "                while (t < nsamps - 2) & (float(rho_hat_even) + float(rho_hat_odd) >= 0):\n",
    "                    # above equation (11.7) in Gelman et al., 2013\n",
    "                    variogram_odd = np.mean(np.mean(np.power(alldata[v,(t-1):nsamps,:] - alldata[v,0:(nsamps-t+1),:],2),axis=0))\n",
    "                    \n",
    "                    # Equation (11.7) in Gelman et al., 2013\n",
    "                    rho_hat_odd = 1 - np.divide(variogram_odd, 2*totalestvar[whereis]).item()\n",
    "                    rho_hat.append(rho_hat_odd)\n",
    "                    \n",
    "                    # above equation (11.7) in Gelman et al., 2013\n",
    "                    variogram_even = np.mean(np.mean(np.power(alldata[v,t:nsamps,:] - alldata[v,0:(nsamps-t),:],2),axis=0)) \n",
    "                    \n",
    "                    # Equation (11.7) in Gelman et al., 2013\n",
    "                    rho_hat_even = 1 - np.divide(variogram_even, 2*totalestvar[whereis]).item() \n",
    "                    rho_hat.append(rho_hat_even)\n",
    "                    \n",
    "                    t += 2\n",
    "                rho_hat = np.asarray(rho_hat)\n",
    "                # Equation (11.8) in Gelman et al., 2013\n",
    "                neff[whereis] = np.divide(nchains*nsamps, 1 + 2*np.sum(rho_hat)) \n",
    "            result[key]['neff'] = np.round(neff)\n",
    "            minneff[keyindx] = np.nanmin(np.round(neff))\n",
    "            keyindx += 1\n",
    "\n",
    "            # Geweke statistic?\n",
    "    # print(\"Maximum old Rhat was %3.2f for variable %s\" % (np.max(maxrhatsold),allkeys[np.argmax(maxrhatsold)]))\n",
    "    maxrhatkey = allkeys[np.argmax(maxrhatsnew)]\n",
    "    maxrhatindx = np.unravel_index( np.argmax(result[maxrhatkey]['rhat']) , result[maxrhatkey]['rhat'].shape)\n",
    "    print(\"Maximum Rhat was %3.2f for variable %s at index %s\" % (np.max(maxrhatsnew), maxrhatkey, maxrhatindx))\n",
    "    minneffkey = allkeys[np.argmin(minneff)]\n",
    "    minneffindx = np.unravel_index( np.argmin(result[minneffkey]['neff']) , result[minneffkey]['neff'].shape)\n",
    "    print(\"Minimum number of effective samples was %d for variable %s at index %s\" % (np.min(minneff), minneffkey, minneffindx))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd8a6cd43afc2407"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def models_diagnostics_dict_to_df(models_diagnostics):\n",
    "    results_df = pd.DataFrame()\n",
    "    for key in models_diagnostics.keys():\n",
    "        main_data = models_diagnostics[key]\n",
    "\n",
    "        if main_data['mean'].ndim == 1:\n",
    "            this_df = pd.DataFrame(\n",
    "                {\n",
    "                    f\"{key}.{i + 1}\": \n",
    "                        [main_data[inner_key][i] for inner_key in main_data.keys()] for i in range(main_data['mean'].shape[0]) \n",
    "                }, index=main_data.keys()\n",
    "            )\n",
    "\n",
    "        elif main_data['mean'].ndim == 2:\n",
    "            this_df = pd.DataFrame(\n",
    "                {\n",
    "                    f\"{key}.{i + 1}.{j + 1}\": \n",
    "                     [main_data[inner_key][i, j] for inner_key in main_data.keys()] for i in range(main_data['mean'].shape[0]) for j in range(main_data['mean'].shape[1])\n",
    "                }, index=main_data.keys()\n",
    "            )\n",
    "        else:\n",
    "            this_df = pd.DataFrame()\n",
    "            print('3-dim parameters are not implemented')\n",
    "    \n",
    "        results_df = pd.concat([results_df, this_df], axis=1)\n",
    "        \n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d2ccbbd4b8ee6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def flip_stan_out(fit, parameters=None):\n",
    "    results = {}\n",
    "    \n",
    "    if parameters is None:\n",
    "        pass\n",
    "    else:\n",
    "        for parameter in parameters:\n",
    "            print(f\"Processing: {parameter} \")\n",
    "            samples = fit[parameter]\n",
    "\n",
    "            # reshape from (n_params, n_samples*n_chains) to (n_params, n_samples, n_chains)\n",
    "            samples_reshaped = samples.reshape(\n",
    "                samples.shape[:-1] + (num_samples, num_chains), \n",
    "                order='C'\n",
    "            )\n",
    "            results[parameter] = samples_reshaped\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69d13092c2f78f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creates a dict [parameter_name] : array of shape (*n_params, n_samples, n_chains)\n",
    "parameters = fit.param_names\n",
    "extracted_samples_dict = flip_stan_out(fit, parameters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a30967652aa074"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show model diagnostics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b754ae6b1f3c6648"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_diagnostics = diagnostic(extracted_samples_dict)\n",
    "models_diagnostics_df = models_diagnostics_dict_to_df(models_diagnostics)\n",
    "display(models_diagnostics_df.T)\n",
    "\n",
    "# save results\n",
    "# models_diagnostics_df.T.to_csv('models_diagnostics.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5fb70325129b90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Posterior distribution plots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddee45233ab37049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adapted from https://github.com/mdnunez/pyhddmjags/tree/master\n",
    "def jellyfish(possamps):  # jellyfish plots\n",
    "    \"\"\"Plots posterior distributions of given posterior samples in a jellyfish\n",
    "    plot. Jellyfish plots are posterior distributions (mirrored over their\n",
    "    horizontal axes) with 99% and 95% credible intervals (currently plotted\n",
    "    from the .5% and 99.5% & 2.5% and 97.5% percentiles respectively.\n",
    "    Also plotted are the median and mean of the posterior distributions\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    possamps : ndarray of posterior chains where the last dimension is\n",
    "    the number of chains, the second to last dimension is the number of samples\n",
    "    in each chain, all other dimensions describe the shape of the parameter\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of chains\n",
    "    nchains = possamps.shape[-1]\n",
    "\n",
    "    # Number of samples per chain\n",
    "    nsamps = possamps.shape[-2]\n",
    "\n",
    "    # Number of dimensions\n",
    "    ndims = possamps.ndim - 2\n",
    "\n",
    "    # Number of variables to plot\n",
    "    nvars = np.prod(possamps.shape[0:-2])\n",
    "\n",
    "    # Index of variables\n",
    "    varindx = np.arange(nvars).reshape(possamps.shape[0:-2])\n",
    "\n",
    "    # Reshape data\n",
    "    alldata = np.reshape(possamps, (nvars, nchains, nsamps))\n",
    "    alldata = np.reshape(alldata, (nvars, nchains * nsamps))\n",
    "\n",
    "    # Plot properties\n",
    "    LineWidths = np.array([2, 5])\n",
    "    teal = np.array([0, .7, .7])\n",
    "    blue = np.array([0, 0, 1])\n",
    "    orange = np.array([1, .3, 0])\n",
    "    Colors = [teal, blue]\n",
    "\n",
    "    # Initialize ylabels list\n",
    "    ylabels = ['']\n",
    "\n",
    "    for v in range(0, nvars):\n",
    "        # Create ylabel\n",
    "        whereis = np.where(varindx == v)\n",
    "        newlabel = ''\n",
    "        for l in range(0, ndims):\n",
    "            newlabel = newlabel + ('_%i' % whereis[l][0])\n",
    "\n",
    "        ylabels.append(newlabel)\n",
    "\n",
    "        # Compute posterior density curves\n",
    "        kde = stats.gaussian_kde(alldata[v, :])\n",
    "        bounds = stats.scoreatpercentile(alldata[v, :], (.5, 2.5, 97.5, 99.5))\n",
    "        for b in range(0, 2):\n",
    "            # Bound by .5th percentile and 99.5th percentile\n",
    "            x = np.linspace(bounds[b], bounds[-1 - b], 100)\n",
    "            p = kde(x)\n",
    "\n",
    "            # Scale distributions down\n",
    "            maxp = np.max(p)\n",
    "\n",
    "            # Plot jellyfish\n",
    "            upper = .25 * p / maxp + v + 1\n",
    "            lower = -.25 * p / maxp + v + 1\n",
    "            lines = plt.plot(x, upper, x, lower)\n",
    "            plt.setp(lines, color=Colors[b], linewidth=LineWidths[b])\n",
    "            if b == 1:\n",
    "                # Mark mode\n",
    "                wheremaxp = np.argmax(p)\n",
    "                mmode = plt.plot(np.array([1., 1.]) * x[wheremaxp],\n",
    "                                 np.array([lower[wheremaxp], upper[wheremaxp]]))\n",
    "                plt.setp(mmode, linewidth=3, color=orange)\n",
    "                # Mark median\n",
    "                mmedian = plt.plot(np.median(alldata[v, :]), v + 1, 'ko')\n",
    "                plt.setp(mmedian, markersize=10, color=[0., 0., 0.])\n",
    "                # Mark mean\n",
    "                mmean = plt.plot(np.mean(alldata[v, :]), v + 1, '*')\n",
    "                plt.setp(mmean, markersize=10, color=teal)\n",
    "\n",
    "    # Display plot\n",
    "    plt.setp(plt.gca(), yticklabels=ylabels, yticks=np.arange(0, nvars + 1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3000926c5aacb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Posterior distributions\n",
    "for parameter in fit.param_names:\n",
    "    plt.figure()\n",
    "    jellyfish(extracted_samples_dict[parameter])\n",
    "    plt.title(f'Posterior distributions of the {parameter}')\n",
    "    plt.savefig(f'distributions_{parameter}.png', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a51c638d5a1d09f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "de5b95999b972429"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
