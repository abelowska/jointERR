{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HDDM stan model fitting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48a67958d0ac9973"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3389f6ef5f25776c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stan\n",
    "import nest_asyncio\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# enables multithreading in jupyter notebook\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da9d19f4990d66f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams['savefig.dpi'] = 300"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e06b9719d00ad538"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stan model code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a71f2b0753ad095"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HDDM_delta_decomposed_ndt_trick_boundary_informed_simple = \"\"\"\n",
    "functions {\n",
    "    real participant_level_diffusion_lpdf(\n",
    "        vector y, \n",
    "        real boundary,\n",
    "        real boundary_ne,\n",
    "        real boundary_pre_acc,\n",
    "        real boundary_ne_pre_acc,\n",
    "        real ndt, \n",
    "        real bias, \n",
    "        real drift, \n",
    "        real drift_cond, \n",
    "        vector condition, \n",
    "        vector pre_ne,\n",
    "        vector pre_acc,\n",
    "        int n_trials\n",
    "    ) {\n",
    "        vector[n_trials] participant_level_likelihood;\n",
    "        \n",
    "        for (t in 1:n_trials) {\n",
    "            if (abs(y[t]) - ndt > 0) {\n",
    "                participant_level_likelihood[t] = diffusion_lpdf(y[t] | boundary  + boundary_ne*pre_ne[t] + boundary_pre_acc*pre_acc[t] + boundary_ne_pre_acc*pre_ne[t]*pre_acc[t], ndt, bias, drift + drift_cond*condition[t]);\n",
    "            } else {\n",
    "                participant_level_likelihood[t] = diffusion_lpdf(ndt | boundary  + boundary_ne*pre_ne[t] + boundary_pre_acc*pre_acc[t] + boundary_ne_pre_acc*pre_ne[t]*pre_acc[t], ndt, bias, drift + drift_cond*condition[t]);\n",
    "            }\n",
    "        }\n",
    "        return(sum(participant_level_likelihood));\n",
    "    }\n",
    "      \n",
    "    /* Wiener diffusion log-PDF for a single response (adapted from brms 1.10.2)\n",
    "    * Arguments:\n",
    "    *   Y: acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    *   boundary: boundary separation parameter > 0\n",
    "    *   ndt: non-decision time parameter > 0\n",
    "    *   bias: initial bias parameter in [0, 1]\n",
    "    *   drift: drift rate parameter\n",
    "    * Returns:\n",
    "    *   a scalar to be added to the log posterior\n",
    "    */\n",
    "    real diffusion_lpdf(real Y, real boundary, real ndt, real bias, real drift) {\n",
    "        if (Y >= 0) {\n",
    "            return wiener_lpdf( abs(Y) | boundary, ndt, bias, drift );\n",
    "        } else {\n",
    "            return wiener_lpdf( abs(Y) | boundary, ndt, 1-bias, -drift );\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> N; // Number of trial-level observations\n",
    "    int<lower=1> n_conditions; // Number of conditions (congruent and incongruent)\n",
    "    int<lower=1> n_participants; // Number of participants\n",
    "\n",
    "    array[n_participants, 2] int participants_trials_slices; // slices TODO\n",
    "    vector[N] y; // acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    vector[N] condition; // Contrast coded condition: -1 for erroneous and 1 for correct response respectively\n",
    "    vector[N] pre_acc; // Contrast coded accuracy on previous trial\n",
    "    vector[N] pre_ne; // centered correct/error negativity on previous trial\n",
    "    array[N] int<lower=1> participant; // Participant index\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0, upper=0.3>[n_participants] participants_ter; // Participant-level Non-decision time\n",
    "    vector<lower=0, upper=3> [n_participants] participants_alpha; // Participant-level Boundary parameter (speed-accuracy tradeoff) // remove bound\n",
    "    // vector<lower=0, upper=1>[n_participants] participants_beta; // Participant-level Start point bias towards choice A\n",
    "    vector[n_participants] participants_delta; // Participant-level drift-rate\n",
    "    vector[n_participants] participants_delta_cond; // Per-participant condition-level drift-rate adjustment \n",
    "        \n",
    "    real<lower=0> ter; // Hierarchical non-decision time\n",
    "    real<lower=0, upper=3> alpha; // Hierarchical boundary parameter (speed-accuracy tradeoff)\n",
    "    // real beta; // Hierarchical start point bias towards choice A\n",
    "    real delta; // Hierarchical drift-rate\n",
    "    real delta_cond; // Hierarchical drift-rate adjustment \n",
    "    \n",
    "    real<lower=0> ter_sd; // Between-participants variability in non-decision time\n",
    "    real<lower=0> alpha_sd; // Between-participants variability in boundary parameter (speed-accuracy tradeoff)\n",
    "    // real<lower=0> beta_sd; // Between-participants variability in start point bias towards choice A\n",
    "    real<lower=0> delta_sd; // Between-participants variability in drift-rate\n",
    "    real<lower=0> delta_cond_sd; // Between-participants variability in effect of condition\n",
    "    \n",
    "    \n",
    "    // Non Hierarchical\n",
    "    real alpha_ne;\n",
    "    real alpha_pre_acc; \n",
    "    real alpha_ne_pre_acc; \n",
    "}\n",
    "\n",
    "model {\n",
    "\n",
    "    // ##########\n",
    "    // Between-participant variability priors\n",
    "    // ##########\n",
    "    ter_sd ~ gamma(.3,1);\n",
    "    alpha_sd ~ gamma(1,1);\n",
    "    delta_sd ~ gamma(1,1);\n",
    "    delta_cond_sd ~ gamma(1,1);\n",
    "\n",
    "    // ##########\n",
    "    // Hierarchical parameters priors\n",
    "    // ##########\n",
    "    ter ~ normal(.1, .2);\n",
    "    alpha ~ normal(1, 1) T[0, 3];\n",
    "    delta ~ normal(0, 2);\n",
    "    delta_cond ~ normal(0, 2);\n",
    "\n",
    "    // ##########\n",
    "    // Non Hierarchical boundary parameters\n",
    "    // ##########\n",
    "    alpha_ne ~ normal(0, 0.1); // try (0, 0.1) or truncate\n",
    "    alpha_pre_acc ~ normal(0,0.2);  // try (0, 0.2) or truncate\n",
    "    alpha_ne_pre_acc ~ normal(0, 0.1); // try (0, 0.1) or truncate\n",
    "\n",
    "\n",
    "    // ##########\n",
    "    // Participant-level DDM parameter priors\n",
    "    // ##########\n",
    "    for (p in 1:n_participants) {\n",
    "\n",
    "        // Participant-level non-decision time\n",
    "        participants_ter[p] ~ normal(ter, ter_sd) T[0, .3];\n",
    "\n",
    "        // Participant-level boundary parameter (speed-accuracy tradeoff)\n",
    "        participants_alpha[p] ~ normal(alpha, alpha_sd) T[0, 3];\n",
    "\n",
    "        //Participant-level drift rate\n",
    "        participants_delta[p] ~ normal(delta, delta_sd);\n",
    "        \n",
    "        //Participant-level condition_adjustment\n",
    "        participants_delta_cond[p] ~ normal(delta_cond, delta_cond_sd);  \n",
    "                \n",
    "        \n",
    "        target += participant_level_diffusion_lpdf( y[participants_trials_slices[p][1]:participants_trials_slices[p][2]] | participants_alpha[p], alpha_ne, alpha_pre_acc, alpha_ne_pre_acc, participants_ter[p], 0.5, participants_delta[p], participants_delta_cond[p], condition[participants_trials_slices[p][1]:participants_trials_slices[p][2]], pre_ne[participants_trials_slices[p][1]:participants_trials_slices[p][2]],pre_acc[participants_trials_slices[p][1]:participants_trials_slices[p][2]], (participants_trials_slices[p][2] - participants_trials_slices[p][1] + 1));         \n",
    "    }\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb594f6cf34ff775"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HDDM_delta_decomposed_ndt_trick_boundary_informed_condition_simple = \"\"\"\n",
    "functions {\n",
    "    real participant_level_diffusion_lpdf(\n",
    "        vector y, \n",
    "        real boundary,\n",
    "        real boundary_cond,\n",
    "        real boundary_ne,\n",
    "        real boundary_pre_acc,\n",
    "        real boundary_ne_pre_acc,\n",
    "        real boundary_ne_cond,\n",
    "        real boundary_pre_acc_cond,\n",
    "        real boundary_ne_pre_acc_cond,\n",
    "        real ndt, \n",
    "        real bias, \n",
    "        real drift, \n",
    "        real drift_cond, \n",
    "        vector condition, \n",
    "        vector pre_ne,\n",
    "        vector pre_acc,\n",
    "        int n_trials\n",
    "    ) {\n",
    "        vector[n_trials] participant_level_likelihood;\n",
    "        \n",
    "        for (t in 1:n_trials) {\n",
    "            if (abs(y[t]) - ndt > 0) {\n",
    "                participant_level_likelihood[t] = diffusion_lpdf(y[t] | boundary  + boundary_cond*condition[t] + boundary_ne*pre_ne[t] + boundary_pre_acc*pre_acc[t] + boundary_ne_pre_acc*pre_ne[t]*pre_acc[t] + boundary_ne_cond*pre_ne[t]*condition[t] + boundary_pre_acc_cond*pre_acc[t]*condition[t] + boundary_ne_pre_acc_cond*pre_ne[t]*pre_acc[t]*condition[t], ndt, bias, drift + drift_cond*condition[t]);\n",
    "            } else {\n",
    "                participant_level_likelihood[t] = diffusion_lpdf(ndt | boundary  + boundary_cond*condition[t] + boundary_ne*pre_ne[t] + boundary_pre_acc*pre_acc[t] + boundary_ne_pre_acc*pre_ne[t]*pre_acc[t] + boundary_ne_cond*pre_ne[t]*condition[t] + boundary_pre_acc_cond*pre_acc[t]*condition[t] + boundary_ne_pre_acc_cond*pre_ne[t]*pre_acc[t]*condition[t], ndt, bias, drift + drift_cond*condition[t]);\n",
    "            }\n",
    "        }\n",
    "        return(sum(participant_level_likelihood));\n",
    "    }\n",
    "      \n",
    "    /* Wiener diffusion log-PDF for a single response (adapted from brms 1.10.2)\n",
    "    * Arguments:\n",
    "    *   Y: acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    *   boundary: boundary separation parameter > 0\n",
    "    *   ndt: non-decision time parameter > 0\n",
    "    *   bias: initial bias parameter in [0, 1]\n",
    "    *   drift: drift rate parameter\n",
    "    * Returns:\n",
    "    *   a scalar to be added to the log posterior\n",
    "    */\n",
    "    real diffusion_lpdf(real Y, real boundary, real ndt, real bias, real drift) {\n",
    "        if (Y >= 0) {\n",
    "            return wiener_lpdf( abs(Y) | boundary, ndt, bias, drift );\n",
    "        } else {\n",
    "            return wiener_lpdf( abs(Y) | boundary, ndt, 1-bias, -drift );\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> N; // Number of trial-level observations\n",
    "    int<lower=1> n_conditions; // Number of conditions (congruent and incongruent)\n",
    "    int<lower=1> n_participants; // Number of participants\n",
    "\n",
    "    array[n_participants, 2] int participants_trials_slices; // slices TODO\n",
    "    vector[N] y; // acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    vector[N] condition; // Contrast coded condition: -1 for erroneous and 1 for correct response respectively\n",
    "    vector[N] pre_acc; // Contrast coded accuracy on previous trial\n",
    "    vector[N] pre_ne; // centered correct/error negativity on previous trial\n",
    "    array[N] int<lower=1> participant; // Participant index\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0, upper=0.3>[n_participants] participants_ter; // Participant-level Non-decision time\n",
    "    vector<lower=0, upper=3>[n_participants] participants_alpha; // Participant-level Boundary parameter (speed-accuracy tradeoff)\n",
    "    vector[n_participants] participants_alpha_cond; // Per-participant condition-level boundary adjustment \n",
    "    vector[n_participants] participants_delta; // Participant-level drift-rate\n",
    "    vector[n_participants] participants_delta_cond; // Per-participant condition-level drift-rate adjustment \n",
    "        \n",
    "    real<lower=0> ter; // Hierarchical non-decision time\n",
    "    real<lower=0, upper=3> alpha; // Hierarchical boundary parameter (speed-accuracy tradeoff)\n",
    "    real alpha_cond;\n",
    "    real delta; // Hierarchical drift-rate\n",
    "    real delta_cond; // Hierarchical drift-rate adjustment \n",
    "    \n",
    "    real<lower=0> ter_sd; // Between-participants variability in non-decision time\n",
    "    real<lower=0> alpha_sd; // Between-participants variability in boundary parameter (speed-accuracy tradeoff)\n",
    "    real<lower=0> alpha_cond_sd;\n",
    "    real<lower=0> delta_sd; // Between-participants variability in drift-rate\n",
    "    real<lower=0> delta_cond_sd; // Between-participants variability in effect of condition\n",
    "    \n",
    "    \n",
    "    // Non Hierarchical\n",
    "    real alpha_ne;\n",
    "    real alpha_pre_acc; \n",
    "    real alpha_ne_pre_acc; \n",
    "    \n",
    "    real alpha_ne_cond;\n",
    "    real alpha_pre_acc_cond; \n",
    "    real alpha_ne_pre_acc_cond; \n",
    "}\n",
    "\n",
    "model {\n",
    "\n",
    "    // ##########\n",
    "    // Between-participant variability priors\n",
    "    // ##########\n",
    "    ter_sd ~ gamma(.3,1);\n",
    "    alpha_sd ~ gamma(1,1);\n",
    "    alpha_cond_sd ~ gamma(1,1); // 0.3\n",
    "    delta_sd ~ gamma(1,1);\n",
    "    delta_cond_sd ~ gamma(1,1);\n",
    "\n",
    "    // ##########\n",
    "    // Hierarchical parameters priors\n",
    "    // ##########\n",
    "    ter ~ normal(.1, .2);\n",
    "    alpha ~ normal(1, 1) T[0, 3];\n",
    "    alpha_cond ~ normal(0, 1);  // 0.2\n",
    "    delta ~ normal(0, 2);\n",
    "    delta_cond ~ normal(0, 2);\n",
    "\n",
    "    // ##########\n",
    "    // Non Hierarchical boundary parameters\n",
    "    // ##########\n",
    "    alpha_ne ~ normal(0, 0.1); \n",
    "    alpha_pre_acc ~ normal(0,0.2);  \n",
    "    alpha_ne_pre_acc ~ normal(0, 0.1);\n",
    "    \n",
    "    alpha_ne_cond ~ normal(0, 0.1);\n",
    "    alpha_pre_acc_cond ~ normal(0,0.2);  \n",
    "    alpha_ne_pre_acc_cond ~ normal(0, 0.1);\n",
    "\n",
    "\n",
    "    // ##########\n",
    "    // Participant-level DDM parameter priors\n",
    "    // ##########\n",
    "    for (p in 1:n_participants) {\n",
    "\n",
    "        // Participant-level non-decision time\n",
    "        participants_ter[p] ~ normal(ter, ter_sd) T[0, .3];\n",
    "\n",
    "        // Participant-level boundary parameter (speed-accuracy tradeoff)\n",
    "        participants_alpha[p] ~ normal(alpha, alpha_sd) T[0, 3];\n",
    "        \n",
    "        participants_alpha_cond[p] ~ normal(alpha_cond, alpha_cond_sd);\n",
    "\n",
    "        //Participant-level drift rate\n",
    "        participants_delta[p] ~ normal(delta, delta_sd);\n",
    "        \n",
    "        //Participant-level condition_adjustment\n",
    "        participants_delta_cond[p] ~ normal(delta_cond, delta_cond_sd);  \n",
    "                \n",
    "        \n",
    "        target += participant_level_diffusion_lpdf( y[participants_trials_slices[p][1]:participants_trials_slices[p][2]] | participants_alpha[p], participants_alpha_cond[p], alpha_ne, alpha_pre_acc, alpha_ne_pre_acc, alpha_ne_cond, alpha_pre_acc_cond, alpha_ne_pre_acc_cond, participants_ter[p], 0.5, participants_delta[p], participants_delta_cond[p], condition[participants_trials_slices[p][1]:participants_trials_slices[p][2]], pre_ne[participants_trials_slices[p][1]:participants_trials_slices[p][2]],pre_acc[participants_trials_slices[p][1]:participants_trials_slices[p][2]], (participants_trials_slices[p][2] - participants_trials_slices[p][1] + 1));         \n",
    "    }\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d845016cd31e4dad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HDDM_delta_decomposed_ndt_trick_boundary_informed_condition_hdd = \"\"\"\n",
    "functions {\n",
    "    real participant_level_diffusion_lpdf(\n",
    "        vector y, \n",
    "        real boundary,\n",
    "        real boundary_cond,\n",
    "        real boundary_ne,\n",
    "        real boundary_pre_acc,\n",
    "        real boundary_ne_pre_acc,\n",
    "        real boundary_ne_cond,\n",
    "        real boundary_pre_acc_cond,\n",
    "        real boundary_ne_pre_acc_cond,\n",
    "        real ndt, \n",
    "        real bias, \n",
    "        real drift, \n",
    "        real drift_cond, \n",
    "        vector condition, \n",
    "        vector pre_ne,\n",
    "        vector pre_acc,\n",
    "        int n_trials\n",
    "    ) {\n",
    "        vector[n_trials] participant_level_likelihood;\n",
    "        \n",
    "        for (t in 1:n_trials) {\n",
    "            if (abs(y[t]) - ndt > 0) {\n",
    "                participant_level_likelihood[t] = diffusion_lpdf(y[t] | boundary  + boundary_cond*condition[t] + boundary_ne*pre_ne[t] + boundary_pre_acc*pre_acc[t] + boundary_ne_pre_acc*pre_ne[t]*pre_acc[t] + boundary_ne_cond*pre_ne[t]*condition[t] + boundary_pre_acc_cond*pre_acc[t]*condition[t] + boundary_ne_pre_acc_cond*pre_ne[t]*pre_acc[t]*condition[t], ndt, bias, drift + drift_cond*condition[t]);\n",
    "            } else {\n",
    "                participant_level_likelihood[t] = diffusion_lpdf(ndt | boundary  + boundary_cond*condition[t] + boundary_ne*pre_ne[t] + boundary_pre_acc*pre_acc[t] + boundary_ne_pre_acc*pre_ne[t]*pre_acc[t] + boundary_ne_cond*pre_ne[t]*condition[t] + boundary_pre_acc_cond*pre_acc[t]*condition[t] + boundary_ne_pre_acc_cond*pre_ne[t]*pre_acc[t]*condition[t], ndt, bias, drift + drift_cond*condition[t]);\n",
    "            }\n",
    "        }\n",
    "        return(sum(participant_level_likelihood));\n",
    "    }\n",
    "      \n",
    "    /* Wiener diffusion log-PDF for a single response (adapted from brms 1.10.2)\n",
    "    * Arguments:\n",
    "    *   Y: acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    *   boundary: boundary separation parameter > 0\n",
    "    *   ndt: non-decision time parameter > 0\n",
    "    *   bias: initial bias parameter in [0, 1]\n",
    "    *   drift: drift rate parameter\n",
    "    * Returns:\n",
    "    *   a scalar to be added to the log posterior\n",
    "    */\n",
    "    real diffusion_lpdf(real Y, real boundary, real ndt, real bias, real drift) {\n",
    "        if (Y >= 0) {\n",
    "            return wiener_lpdf( abs(Y) | boundary, ndt, bias, drift );\n",
    "        } else {\n",
    "            return wiener_lpdf( abs(Y) | boundary, ndt, 1-bias, -drift );\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "data {\n",
    "    int<lower=1> N; // Number of trial-level observations\n",
    "    int<lower=1> n_conditions; // Number of conditions (congruent and incongruent)\n",
    "    int<lower=1> n_participants; // Number of participants\n",
    "\n",
    "    array[n_participants, 2] int participants_trials_slices; // slices TODO\n",
    "    vector[N] y; // acc*rt in seconds (negative and positive RTs for incorrect and correct responses respectively)\n",
    "    vector[N] condition; // Contrast coded condition: -1 for erroneous and 1 for correct response respectively\n",
    "    vector[N] pre_acc; // Contrast coded accuracy on previous trial\n",
    "    vector[N] pre_ne; // centered correct/error negativity on previous trial\n",
    "    array[N] int<lower=1> participant; // Participant index\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0, upper=0.3>[n_participants] participants_ter; // Participant-level Non-decision time\n",
    "    vector<lower=0, upper=3>[n_participants] participants_alpha; // Participant-level Boundary parameter (speed-accuracy tradeoff)\n",
    "    vector[n_participants] participants_alpha_cond; // Per-participant condition-level boundary adjustment\n",
    "    vector[n_participants] participants_alpha_ne;  \n",
    "    vector[n_participants] participants_alpha_pre_acc; \n",
    "    vector[n_participants] participants_alpha_ne_pre_acc; \n",
    "    vector[n_participants] participants_alpha_ne_cond; \n",
    "    vector[n_participants] participants_alpha_pre_acc_cond; \n",
    "    vector[n_participants] participants_alpha_ne_pre_acc_cond; \n",
    " \n",
    "    vector[n_participants] participants_delta; // Participant-level drift-rate\n",
    "    vector[n_participants] participants_delta_cond; // Per-participant condition-level drift-rate adjustment \n",
    "        \n",
    "    real<lower=0> ter; // Hierarchical non-decision time\n",
    "    real<lower=0, upper=3> alpha; // Hierarchical boundary parameter (speed-accuracy tradeoff)\n",
    "    real alpha_cond;\n",
    "    real delta; // Hierarchical drift-rate\n",
    "    real delta_cond; // Hierarchical drift-rate adjustment \n",
    "    \n",
    "    real alpha_ne;\n",
    "    real alpha_pre_acc; \n",
    "    real alpha_ne_pre_acc; \n",
    "    real alpha_ne_cond;\n",
    "    real alpha_pre_acc_cond; \n",
    "    real alpha_ne_pre_acc_cond; \n",
    "    \n",
    "    real<lower=0> ter_sd; // Between-participants variability in non-decision time\n",
    "    real<lower=0> alpha_sd; // Between-participants variability in boundary parameter (speed-accuracy tradeoff)\n",
    "    real<lower=0> alpha_cond_sd;\n",
    "    real<lower=0> alpha_ne_sd;\n",
    "    real<lower=0> alpha_pre_acc_sd;\n",
    "    real<lower=0> alpha_ne_pre_acc_sd;\n",
    "    real<lower=0> alpha_ne_cond_sd;\n",
    "    real<lower=0> alpha_pre_acc_cond_sd;\n",
    "    real<lower=0> alpha_ne_pre_acc_cond_sd;\n",
    "\n",
    "    real<lower=0> delta_sd; // Between-participants variability in drift-rate\n",
    "    real<lower=0> delta_cond_sd; // Between-participants variability in effect of condition\n",
    "    \n",
    "}\n",
    "\n",
    "model {\n",
    "\n",
    "    // ##########\n",
    "    // Between-participant variability priors\n",
    "    // ##########\n",
    "    ter_sd ~ gamma(.3,1);\n",
    "    alpha_sd ~ gamma(1,1);\n",
    "    alpha_cond_sd ~ gamma(1,1); // 0.3\n",
    "    \n",
    "    alpha_ne_sd ~ gamma(1,1); // works quite nice with 0.3, 1 and really nice with (1,1)\n",
    "    alpha_pre_acc_sd ~ gamma(1,1); // works with 1, 1 and really nice with (1,1)\n",
    "    alpha_ne_pre_acc_sd ~ gamma(1,1); // works with 0.3, 1 and really nice with (1,1)\n",
    "    alpha_ne_cond_sd ~ gamma(1,1); // works with 0.3, 1 and really nice with (1,1)\n",
    "    alpha_pre_acc_cond_sd ~ gamma(1,1); // works with 1, 1 and really nice with (1,1)\n",
    "    alpha_ne_pre_acc_cond_sd ~ gamma(1,1); // works with 0.3, 1 and really nice with (1,1)\n",
    "\n",
    "    delta_sd ~ gamma(1,1);\n",
    "    delta_cond_sd ~ gamma(1,1);\n",
    "\n",
    "    // ##########\n",
    "    // Hierarchical parameters priors\n",
    "    // ##########\n",
    "    ter ~ normal(.1, .2);\n",
    "    alpha ~ normal(1, 1) T[0, 3];\n",
    "    alpha_cond ~ normal(0, 1);  \n",
    "    delta ~ normal(0, 2);\n",
    "    delta_cond ~ normal(0, 2);\n",
    "\n",
    "    alpha_ne ~ normal(0, 0.5); // was 0.5 and works super, with 1 worser\n",
    "    alpha_pre_acc ~ normal(0, 0.5);  // was 0.5 and works super, with 1 worser\n",
    "    alpha_ne_pre_acc ~ normal(0, 0.5); // was 0.5 and works super, with 1 worser\n",
    "    \n",
    "    alpha_ne_cond ~ normal(0, 0.5); // was 0.5 and works super, with 1 worser\n",
    "    alpha_pre_acc_cond ~ normal(0, 0.5); // was 0.5 and works super, with 1 worser \n",
    "    alpha_ne_pre_acc_cond ~ normal(0, 0.5); // was 0.5 and works super, with 1 worser\n",
    "\n",
    "\n",
    "    // ##########\n",
    "    // Participant-level DDM parameter priors\n",
    "    // ##########\n",
    "    for (p in 1:n_participants) {\n",
    "\n",
    "        // Participant-level non-decision time\n",
    "        participants_ter[p] ~ normal(ter, ter_sd) T[0, .3];\n",
    "\n",
    "        // Participant-level boundary parameter (speed-accuracy tradeoff)\n",
    "        participants_alpha[p] ~ normal(alpha, alpha_sd) T[0, 3];\n",
    "        \n",
    "        participants_alpha_cond[p] ~ normal(alpha_cond, alpha_cond_sd);\n",
    "\n",
    "        //Participant-level drift rate\n",
    "        participants_delta[p] ~ normal(delta, delta_sd);\n",
    "        \n",
    "        //Participant-level condition_adjustment\n",
    "        participants_delta_cond[p] ~ normal(delta_cond, delta_cond_sd);  \n",
    "        \n",
    "        participants_alpha_ne[p] ~ normal(alpha_ne, alpha_ne_sd);\n",
    "        participants_alpha_pre_acc[p] ~ normal(alpha_pre_acc, alpha_pre_acc_sd);\n",
    "        participants_alpha_ne_pre_acc[p] ~ normal(alpha_ne_pre_acc, alpha_ne_pre_acc_sd);\n",
    "        participants_alpha_ne_cond[p] ~ normal(alpha_ne_cond, alpha_ne_cond_sd);\n",
    "        participants_alpha_pre_acc_cond[p] ~ normal(alpha_pre_acc_cond, alpha_pre_acc_cond_sd);\n",
    "        participants_alpha_ne_pre_acc_cond[p] ~ normal(alpha_ne_pre_acc_cond, alpha_ne_pre_acc_cond_sd);\n",
    "\n",
    "                \n",
    "        \n",
    "        target += participant_level_diffusion_lpdf( y[participants_trials_slices[p][1]:participants_trials_slices[p][2]] | participants_alpha[p], participants_alpha_cond[p], participants_alpha_ne[p], participants_alpha_pre_acc[p], participants_alpha_ne_pre_acc[p], participants_alpha_ne_cond[p], participants_alpha_pre_acc_cond[p], participants_alpha_ne_pre_acc_cond[p], participants_ter[p], 0.5, participants_delta[p], participants_delta_cond[p], condition[participants_trials_slices[p][1]:participants_trials_slices[p][2]], pre_ne[participants_trials_slices[p][1]:participants_trials_slices[p][2]],pre_acc[participants_trials_slices[p][1]:participants_trials_slices[p][2]], (participants_trials_slices[p][2] - participants_trials_slices[p][1] + 1));         \n",
    "    }\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a42a699de75c882"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read and prepare data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51eba714e6303334"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('twentythree_participants_post_eeg_many_test_set.csv').drop(columns='Unnamed: 0')\n",
    "\n",
    "# check dataframe\n",
    "display(df.isnull().any())\n",
    "display(df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f294014ec1d9970"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove trials with NaNs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40dac6d7f1f3ff0c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_no_nans = df.dropna()\n",
    "\n",
    "# check dataframe\n",
    "display(df_no_nans.isnull().any())\n",
    "display(df_no_nans)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cc574de78bf20c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove trials with RT < 100ms for model to converge (problem with non-decision time)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77399b0e069d8339"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "df_rts_truncated = df_no_nans[df_no_nans['rt'] > threshold]\n",
    "\n",
    "df_rts_truncated"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edb03f9f1174237b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter CCXP trials sequences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec750d933898a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_rts_truncated = df_rts_truncated[df_rts_truncated['is_in_sequence'] == True]\n",
    "# df_rts_truncated"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "961422c3c1b06268"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare 1D data for Stan with information on per participant number of trials"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf6005bce1f136b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = df_rts_truncated['y'].to_numpy()\n",
    "condition = df_rts_truncated['condition'].to_numpy()\n",
    "pre_acc = df_rts_truncated['pre_acc'].to_numpy()\n",
    "# pre_ne = df_rts_truncated['pre_ne_FCz_centered'].to_numpy()\n",
    "participant_index = df_rts_truncated['participant_index'].to_numpy()\n",
    "\n",
    "n_participants = len(np.unique(participant_index))\n",
    "n_conditions = len(np.unique(condition))\n",
    "\n",
    "participants_trials_slices = []\n",
    "pre_ne = []\n",
    "for index in np.unique(participant_index):\n",
    "    indices = np.where(participant_index == index)[0]\n",
    "    start_index = indices[0] + 1\n",
    "    end_index = indices[-1] + 1\n",
    "    participants_trials_slices.append([start_index, end_index])\n",
    "    \n",
    "    participants_ne = df_rts_truncated.iloc[indices]['pre_ne_FCz'].to_numpy().flatten()\n",
    "    participants_ne_stand = (participants_ne - np.mean(participants_ne)) / np.std(participants_ne)\n",
    "    \n",
    "    pre_ne.extend(participants_ne_stand)\n",
    "    \n",
    "participants_trials_slices = np.array(participants_trials_slices)\n",
    "pre_ne = np.array(pre_ne)\n",
    "df_rts_truncated['pre_ne_FCz_centered'] = pre_ne"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "49f515432e6b23b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check distributions of EEG"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb49fb060367216f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(pre_ne)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "190fb0c72ceaca63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(\n",
    "    df_rts_truncated.sort_values(['ID']),\n",
    "    col=\"ID\",\n",
    "    col_wrap=2,\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    aspect=2,\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.histplot,\n",
    "    x=\"pre_ne_FCz_centered\",\n",
    "    hue='pre_acc',\n",
    "    kde=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f63284c1ac0b00e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Shape of y data: {y.shape}\")\n",
    "print(f\"Shape of condition data: {condition.shape}\")\n",
    "print(f\"Number of participants: {n_participants}\\nNumber of conditions: {n_conditions}\")\n",
    "print(f\"Participants trial slices shape: {participants_trials_slices.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5a724e4c7fe7d2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_2d = {\n",
    "    \"N\": len(y),\n",
    "    \"participants_trials_slices\": participants_trials_slices,\n",
    "    \"n_conditions\": n_conditions,\n",
    "    \"n_participants\": n_participants,\n",
    "    \"y\": y,\n",
    "    \"condition\": condition,\n",
    "    'pre_ne': pre_ne,\n",
    "    'pre_acc': pre_acc,\n",
    "    \"participant\": participant_index,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345d01420756bb84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and fit the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f279e6c638c44b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "posterior = stan.build(HDDM_delta_decomposed_ndt_trick_boundary_informed_condition_hdd, data=data_2d, random_seed=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c9b9a061e0759a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_chains = 4\n",
    "warmup = 2000\n",
    "num_samples = 1000\n",
    "thin=1\n",
    "\n",
    "min_rt = np.zeros(n_participants)\n",
    "for idx, participant_idx in enumerate(np.unique(participant_index)):\n",
    "    participant_data = df_rts_truncated[df_rts_truncated['participant_index'] == participant_idx]['y'].to_numpy()\n",
    "    min_rt[idx] = np.min(abs(participant_data))\n",
    "\n",
    "initials = []\n",
    "for c in range(0, num_chains):\n",
    "    chain_init = {               \n",
    "        'ter_sd': np.random.uniform(.01, .2),\n",
    "        'alpha_sd': np.random.uniform(.01, 1.),\n",
    "        'alpha_cond_sd': np.random.uniform(.01, 1.),\n",
    "        'delta_sd': np.random.uniform(.1, 3.),\n",
    "        'delta_cond_sd': np.random.uniform(.1, 3.),\n",
    "        \n",
    "        'alpha_ne_sd': np.random.uniform(.01, 1), \n",
    "        'alpha_pre_acc_sd': np.random.uniform(.01, 1), \n",
    "        'alpha_ne_pre_acc_sd': np.random.uniform(.01, 1), \n",
    "        'alpha_ne_cond_sd': np.random.uniform(.01, 1), \n",
    "        'alpha_pre_acc_cond_sd': np.random.uniform(.01, 1), \n",
    "        'alpha_ne_pre_acc_cond_sd': np.random.uniform(.01, 1),\n",
    "\n",
    "\n",
    "        'ter': np.random.uniform(0.05, .3),\n",
    "        'alpha': np.random.uniform(1, 2), \n",
    "        'alpha_cond': np.random.uniform(-.5, .5), \n",
    "        'delta': np.random.uniform(-4., 4.),\n",
    "        'delta_cond': np.random.uniform(-4., 4.),\n",
    "\n",
    "        'alpha_ne': np.random.uniform(-.05, .05), \n",
    "        'alpha_pre_acc': np.random.uniform(-0.1, .1), \n",
    "        'alpha_ne_pre_acc': np.random.uniform(-.05, .05), \n",
    "        'alpha_ne_cond': np.random.uniform(-.05, .05), \n",
    "        'alpha_pre_acc_cond': np.random.uniform(-0.1, .1), \n",
    "        'alpha_ne_pre_acc_cond': np.random.uniform(-.05, .05),\n",
    "        \n",
    "        'participants_ter': np.random.uniform(0.05, .3, size=n_participants),\n",
    "        'participants_alpha': np.random.uniform(1, 2., size=n_participants), \n",
    "        'participants_alpha_cond': np.random.uniform(-0.5, .5, size=n_participants),\n",
    "        'participants_delta': np.random.uniform(-4., 4., size=n_participants),\n",
    "        'participants_delta_cond': np.random.uniform(-4., 4., size=n_participants),\n",
    "        \n",
    "        'participants_alpha_ne': np.random.uniform(-.05, .05, size=n_participants), \n",
    "        'participants_alpha_pre_acc': np.random.uniform(-0.1, .1, size=n_participants), \n",
    "        'participants_alpha_ne_pre_acc': np.random.uniform(-.05, .05, size=n_participants), \n",
    "        'participants_alpha_ne_cond': np.random.uniform(-.05, .05, size=n_participants),  \n",
    "        'participants_alpha_pre_acc_cond': np.random.uniform(-0.1, .1, size=n_participants),   \n",
    "        'participants_alpha_ne_pre_acc_cond': np.random.uniform(-.05, .05, size=n_participants), \n",
    "    }\n",
    "\n",
    "    for p in range(0, n_participants):\n",
    "        chain_init['participants_ter'][p] = np.random.uniform(0., min_rt[p]/2)\n",
    "\n",
    "    initials.append(chain_init)\n",
    "\n",
    "print(min_rt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e48a90e88d706496"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fit = posterior.sample(\n",
    "    num_chains=num_chains, \n",
    "    num_samples=num_samples, \n",
    "    num_warmup = warmup, \n",
    "    save_warmup=False, \n",
    "    init=initials, \n",
    "    num_thin=thin\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83368af9ca2e95d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract samples and chains"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc5eafb84fe80cf2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fit_df = fit.to_frame()\n",
    "\n",
    "# adds chain number to dataframe with draws_. See: https://github.com/stan-dev/pystan/pull/333\n",
    "samples_saved = num_samples // thin\n",
    "chains = np.ones((samples_saved, 1), int) * np.arange(num_chains)\n",
    "fit_df.insert(0, \"chain__\", chains.ravel())\n",
    "\n",
    "fit_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48e921eab284ca27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f92d5e2ac59b6f95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Summary of the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a080f3f5fe2795a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "variables_to_track = list(posterior.constrained_param_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71b7d7d3243bcaf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# overall summary\n",
    "fit_df[variables_to_track].describe().T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ea178d73017cc3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# summary by chain\n",
    "fit_df.groupby(['chain__'])[variables_to_track].describe().T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9624f4a15fb963e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Posterior and chains plots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0782e5142329116"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(50,100))\n",
    "\n",
    "melted_df = pd.melt(fit_df, id_vars=list(filter(lambda x: x not in set(variables_to_track),fit_df.columns.to_list())), var_name='parameter_name', value_name='draws')\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    melted_df,\n",
    "    col=\"parameter_name\",\n",
    "    col_wrap=3,\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    aspect=1.5,\n",
    "    hue='chain__',\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.histplot,\n",
    "    x=\"draws\",\n",
    "    kde=True,\n",
    ")\n",
    "\n",
    "g.add_legend()\n",
    "# plt.savefig('hddm_parameters_posteriors_trick_cutoff.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(\n",
    "    melted_df,\n",
    "    col=\"parameter_name\",\n",
    "    col_wrap=3,\n",
    "    sharex=False,\n",
    "    sharey=False,\n",
    "    aspect=1.5,\n",
    "    hue='chain__',\n",
    ")\n",
    "\n",
    "g.map_dataframe(\n",
    "    sns.lineplot,\n",
    "    x=np.arange(0,samples_saved),\n",
    "    y=\"draws\",\n",
    ")\n",
    "\n",
    "g.add_legend()\n",
    "# plt.savefig('hddm_chains_trick_cutoff.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35bd4e20c0640420"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Diagnostics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15fb5d61df99542d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adapted from https://github.com/mdnunez/pyhddmjags/tree/master\n",
    "def diagnostic(insamples):\n",
    "    \"\"\"\n",
    "    Returns two versions of Rhat (measure of convergence, less is better with an approximate\n",
    "    1.10 cutoff) and Neff, number of effective samples). Note that 'rhat' is more diagnostic than 'oldrhat' according to \n",
    "    Gelman et al. (2014).\n",
    "\n",
    "    Reference for preferred Rhat calculation (split chains) and number of effective sample calculation: \n",
    "        Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A. & Rubin, D. B. (2014). \n",
    "        Bayesian data analysis (Third Edition). CRC Press:\n",
    "        Boca Raton, FL\n",
    "\n",
    "    Reference for original Rhat calculation:\n",
    "        Gelman, A., Carlin, J., Stern, H., & Rubin D., (2004).\n",
    "        Bayesian Data Analysis (Second Edition). Chapman & Hall/CRC:\n",
    "        Boca Raton, FL.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    insamples: dic\n",
    "        Sampled values of monitored variables as a dictionary where keys\n",
    "        are variable names and values are numpy arrays with shape:\n",
    "        (dim_1, dim_n, iterations, chains). dim_1, ..., dim_n describe the\n",
    "        shape of variable in JAGS model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict:\n",
    "        rhat, oldrhat, neff, posterior mean, and posterior std for each variable. Prints maximum Rhat and minimum Neff across all variables\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}  # Initialize dictionary\n",
    "    maxrhatsold = np.zeros((len(insamples.keys())), dtype=float)\n",
    "    maxrhatsnew = np.zeros((len(insamples.keys())), dtype=float)\n",
    "    minneff = np.ones((len(insamples.keys())), dtype=float)*np.inf\n",
    "    allkeys ={} # Initialize dictionary\n",
    "    keyindx = 0\n",
    "    for key in insamples.keys():\n",
    "        if key[0] != '_':\n",
    "            result[key] = {}\n",
    "\n",
    "            possamps = insamples[key]\n",
    "\n",
    "            # Number of chains\n",
    "            nchains = possamps.shape[-1]\n",
    "\n",
    "            # Number of samples per chain\n",
    "            nsamps = possamps.shape[-2]\n",
    "\n",
    "            # Number of variables per key\n",
    "            nvars = np.prod(possamps.shape[0:-2])\n",
    "\n",
    "            # Reshape data\n",
    "            allsamps = np.reshape(possamps, possamps.shape[:-2] + (nchains * nsamps,))\n",
    "\n",
    "            # Reshape data to preduce R_hatnew\n",
    "            possampsnew = np.empty(possamps.shape[:-2] + (int(nsamps/2), nchains * 2,))\n",
    "            newc=0\n",
    "            for c in range(nchains):\n",
    "                possampsnew[...,newc] = np.take(np.take(possamps,np.arange(0,int(nsamps/2)),axis=-2),c,axis=-1)\n",
    "                possampsnew[...,newc+1] = np.take(np.take(possamps,np.arange(int(nsamps/2),nsamps),axis=-2),c,axis=-1)\n",
    "                newc += 2\n",
    "\n",
    "            # Index of variables\n",
    "            varindx = np.arange(nvars).reshape(possamps.shape[0:-2])\n",
    "\n",
    "            # Reshape data\n",
    "            alldata = np.reshape(possamps, (nvars, nsamps, nchains))\n",
    "\n",
    "            # Mean of each chain for rhat\n",
    "            chainmeans = np.mean(possamps, axis=-2)\n",
    "            # Mean of each chain for rhatnew\n",
    "            chainmeansnew = np.mean(possampsnew, axis=-2)\n",
    "            # Global mean of each parameter for rhat\n",
    "            globalmean = np.mean(chainmeans, axis=-1)\n",
    "            globalmeannew = np.mean(chainmeansnew, axis=-1)\n",
    "            result[key]['mean'] = globalmean\n",
    "            result[key]['std'] = np.std(allsamps, axis=-1)\n",
    "            globalmeanext = np.expand_dims(\n",
    "                globalmean, axis=-1)  # Expand the last dimension\n",
    "            globalmeanext = np.repeat(\n",
    "                globalmeanext, nchains, axis=-1)  # For differencing\n",
    "            globalmeanextnew = np.expand_dims(\n",
    "                globalmeannew, axis=-1)  # Expand the last dimension\n",
    "            globalmeanextnew = np.repeat(\n",
    "                globalmeanextnew, nchains*2, axis=-1)  # For differencing\n",
    "            # Between-chain variance for rhat\n",
    "            between = np.sum(np.square(chainmeans - globalmeanext),\n",
    "                             axis=-1) * nsamps / (nchains - 1.)\n",
    "            # Mean of the variances of each chain for rhat\n",
    "            within = np.mean(np.var(possamps, axis=-2), axis=-1)\n",
    "            # Total estimated variance for rhat\n",
    "            totalestvar = (1. - (1. / nsamps)) * \\\n",
    "                          within + (1. / nsamps) * between\n",
    "            # Rhat (original Gelman-Rubin statistic)\n",
    "            temprhat = np.sqrt(totalestvar / within)\n",
    "            maxrhatsold[keyindx] = np.nanmax(temprhat) # Ignore NANs\n",
    "            allkeys[keyindx] = key\n",
    "            result[key]['oldrhat'] = temprhat\n",
    "            # Between-chain variance for rhatnew\n",
    "            betweennew = np.sum(np.square(chainmeansnew - globalmeanextnew),\n",
    "                                axis=-1) * (nsamps/2) / ((nchains*2) - 1.)\n",
    "            # Mean of the variances of each chain for rhatnew\n",
    "            withinnew = np.mean(np.var(possampsnew, axis=-2), axis=-1)\n",
    "            # Total estimated variance\n",
    "            totalestvarnew = (1. - (1. / (nsamps/2))) * \\\n",
    "                             withinnew + (1. / (nsamps/2)) * betweennew\n",
    "            # Rhatnew (Gelman-Rubin statistic from Gelman et al., 2013)\n",
    "            temprhatnew = np.sqrt(totalestvarnew / withinnew)\n",
    "            maxrhatsnew[keyindx] = np.nanmax(temprhatnew) # Ignore NANs\n",
    "            result[key]['rhat'] = temprhatnew\n",
    "            # Number of effective samples from Gelman et al. (2013) 286-288\n",
    "            neff = np.empty(possamps.shape[0:-2])\n",
    "            for v in range(0, nvars):\n",
    "                whereis = np.where(varindx == v)\n",
    "                rho_hat = []\n",
    "                rho_hat_even = 0\n",
    "                rho_hat_odd = 0\n",
    "                t = 2\n",
    "                while (t < nsamps - 2) & (float(rho_hat_even) + float(rho_hat_odd) >= 0):\n",
    "                    # above equation (11.7) in Gelman et al., 2013\n",
    "                    variogram_odd = np.mean(np.mean(np.power(alldata[v,(t-1):nsamps,:] - alldata[v,0:(nsamps-t+1),:],2),axis=0))\n",
    "                    \n",
    "                    # Equation (11.7) in Gelman et al., 2013\n",
    "                    rho_hat_odd = 1 - np.divide(variogram_odd, 2*totalestvar[whereis]).item()\n",
    "                    rho_hat.append(rho_hat_odd)\n",
    "                    \n",
    "                    # above equation (11.7) in Gelman et al., 2013\n",
    "                    variogram_even = np.mean(np.mean(np.power(alldata[v,t:nsamps,:] - alldata[v,0:(nsamps-t),:],2),axis=0)) \n",
    "                    \n",
    "                    # Equation (11.7) in Gelman et al., 2013\n",
    "                    rho_hat_even = 1 - np.divide(variogram_even, 2*totalestvar[whereis]).item() \n",
    "                    rho_hat.append(rho_hat_even)\n",
    "                    \n",
    "                    t += 2\n",
    "                rho_hat = np.asarray(rho_hat)\n",
    "                # Equation (11.8) in Gelman et al., 2013\n",
    "                neff[whereis] = np.divide(nchains*nsamps, 1 + 2*np.sum(rho_hat)) \n",
    "            result[key]['neff'] = np.round(neff)\n",
    "            minneff[keyindx] = np.nanmin(np.round(neff))\n",
    "            keyindx += 1\n",
    "\n",
    "            # Geweke statistic?\n",
    "    # print(\"Maximum old Rhat was %3.2f for variable %s\" % (np.max(maxrhatsold),allkeys[np.argmax(maxrhatsold)]))\n",
    "    maxrhatkey = allkeys[np.argmax(maxrhatsnew)]\n",
    "    maxrhatindx = np.unravel_index( np.argmax(result[maxrhatkey]['rhat']) , result[maxrhatkey]['rhat'].shape)\n",
    "    print(\"Maximum Rhat was %3.2f for variable %s at index %s\" % (np.max(maxrhatsnew), maxrhatkey, maxrhatindx))\n",
    "    minneffkey = allkeys[np.argmin(minneff)]\n",
    "    minneffindx = np.unravel_index( np.argmin(result[minneffkey]['neff']) , result[minneffkey]['neff'].shape)\n",
    "    print(\"Minimum number of effective samples was %d for variable %s at index %s\" % (np.min(minneff), minneffkey, minneffindx))\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd8a6cd43afc2407"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def models_diagnostics_dict_to_df(models_diagnostics):\n",
    "    results_df = pd.DataFrame()\n",
    "    for key in models_diagnostics.keys():\n",
    "        main_data = models_diagnostics[key]\n",
    "\n",
    "        if main_data['mean'].ndim == 1:\n",
    "            this_df = pd.DataFrame(\n",
    "                {\n",
    "                    f\"{key}.{i + 1}\": \n",
    "                        [main_data[inner_key][i] for inner_key in main_data.keys()] for i in range(main_data['mean'].shape[0]) \n",
    "                }, index=main_data.keys()\n",
    "            )\n",
    "\n",
    "        elif main_data['mean'].ndim == 2:\n",
    "            this_df = pd.DataFrame(\n",
    "                {\n",
    "                    f\"{key}.{i + 1}.{j + 1}\": \n",
    "                     [main_data[inner_key][i, j] for inner_key in main_data.keys()] for i in range(main_data['mean'].shape[0]) for j in range(main_data['mean'].shape[1])\n",
    "                }, index=main_data.keys()\n",
    "            )\n",
    "        else:\n",
    "            this_df = pd.DataFrame()\n",
    "            print('3-dim parameters are not implemented')\n",
    "    \n",
    "        results_df = pd.concat([results_df, this_df], axis=1)\n",
    "        \n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d2ccbbd4b8ee6a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def flip_stan_out(fit, parameters=None):\n",
    "    results = {}\n",
    "    \n",
    "    if parameters is None:\n",
    "        pass\n",
    "    else:\n",
    "        for parameter in parameters:\n",
    "            print(f\"Processing: {parameter} \")\n",
    "            samples = fit[parameter]\n",
    "\n",
    "            # reshape from (n_params, n_samples*n_chains) to (n_params, n_samples, n_chains)\n",
    "            samples_reshaped = samples.reshape(\n",
    "                samples.shape[:-1] + (num_samples, num_chains), \n",
    "                order='C'\n",
    "            )\n",
    "            results[parameter] = samples_reshaped\n",
    "    \n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c69d13092c2f78f1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creates a dict [parameter_name] : array of shape (*n_params, n_samples, n_chains)\n",
    "parameters = fit.param_names\n",
    "extracted_samples_dict = flip_stan_out(fit, parameters)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a30967652aa074"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show model diagnostics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b754ae6b1f3c6648"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_diagnostics = diagnostic(extracted_samples_dict)\n",
    "models_diagnostics_df = models_diagnostics_dict_to_df(models_diagnostics)\n",
    "models_diagnostics_df.T\n",
    "\n",
    "# save results\n",
    "# models_diagnostics_df.T.to_csv('hddm_model_trick_cutoff_diagnostics.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5fb70325129b90b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Posterior distribution plots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddee45233ab37049"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adapted from https://github.com/mdnunez/pyhddmjags/tree/master\n",
    "def jellyfish(possamps):  # jellyfish plots\n",
    "    \"\"\"Plots posterior distributions of given posterior samples in a jellyfish\n",
    "    plot. Jellyfish plots are posterior distributions (mirrored over their\n",
    "    horizontal axes) with 99% and 95% credible intervals (currently plotted\n",
    "    from the .5% and 99.5% & 2.5% and 97.5% percentiles respectively.\n",
    "    Also plotted are the median and mean of the posterior distributions\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    possamps : ndarray of posterior chains where the last dimension is\n",
    "    the number of chains, the second to last dimension is the number of samples\n",
    "    in each chain, all other dimensions describe the shape of the parameter\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of chains\n",
    "    nchains = possamps.shape[-1]\n",
    "\n",
    "    # Number of samples per chain\n",
    "    nsamps = possamps.shape[-2]\n",
    "\n",
    "    # Number of dimensions\n",
    "    ndims = possamps.ndim - 2\n",
    "\n",
    "    # Number of variables to plot\n",
    "    nvars = np.prod(possamps.shape[0:-2])\n",
    "\n",
    "    # Index of variables\n",
    "    varindx = np.arange(nvars).reshape(possamps.shape[0:-2])\n",
    "\n",
    "    # Reshape data\n",
    "    alldata = np.reshape(possamps, (nvars, nchains, nsamps))\n",
    "    alldata = np.reshape(alldata, (nvars, nchains * nsamps))\n",
    "\n",
    "    # Plot properties\n",
    "    LineWidths = np.array([2, 5])\n",
    "    teal = np.array([0, .7, .7])\n",
    "    blue = np.array([0, 0, 1])\n",
    "    orange = np.array([1, .3, 0])\n",
    "    Colors = [teal, blue]\n",
    "\n",
    "    # Initialize ylabels list\n",
    "    ylabels = ['']\n",
    "\n",
    "    for v in range(0, nvars):\n",
    "        # Create ylabel\n",
    "        whereis = np.where(varindx == v)\n",
    "        newlabel = ''\n",
    "        for l in range(0, ndims):\n",
    "            newlabel = newlabel + ('_%i' % whereis[l][0])\n",
    "\n",
    "        ylabels.append(newlabel)\n",
    "\n",
    "        # Compute posterior density curves\n",
    "        kde = stats.gaussian_kde(alldata[v, :])\n",
    "        bounds = stats.scoreatpercentile(alldata[v, :], (.5, 2.5, 97.5, 99.5))\n",
    "        for b in range(0, 2):\n",
    "            # Bound by .5th percentile and 99.5th percentile\n",
    "            x = np.linspace(bounds[b], bounds[-1 - b], 100)\n",
    "            p = kde(x)\n",
    "\n",
    "            # Scale distributions down\n",
    "            maxp = np.max(p)\n",
    "\n",
    "            # Plot jellyfish\n",
    "            upper = .25 * p / maxp + v + 1\n",
    "            lower = -.25 * p / maxp + v + 1\n",
    "            lines = plt.plot(x, upper, x, lower)\n",
    "            plt.setp(lines, color=Colors[b], linewidth=LineWidths[b])\n",
    "            if b == 1:\n",
    "                # Mark mode\n",
    "                wheremaxp = np.argmax(p)\n",
    "                mmode = plt.plot(np.array([1., 1.]) * x[wheremaxp],\n",
    "                                 np.array([lower[wheremaxp], upper[wheremaxp]]))\n",
    "                plt.setp(mmode, linewidth=3, color=orange)\n",
    "                # Mark median\n",
    "                mmedian = plt.plot(np.median(alldata[v, :]), v + 1, 'ko')\n",
    "                plt.setp(mmedian, markersize=10, color=[0., 0., 0.])\n",
    "                # Mark mean\n",
    "                mmean = plt.plot(np.mean(alldata[v, :]), v + 1, '*')\n",
    "                plt.setp(mmean, markersize=10, color=teal)\n",
    "\n",
    "    # Display plot\n",
    "    plt.setp(plt.gca(), yticklabels=ylabels, yticks=np.arange(0, nvars + 1))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3000926c5aacb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Posterior distributions\n",
    "for parameter in fit.param_names:\n",
    "    plt.figure()\n",
    "    jellyfish(extracted_samples_dict[parameter])\n",
    "    plt.title(f'Posterior distributions of the {parameter}')\n",
    "    # plt.savefig(f'hddm_distributions_trick_cutoff{parameter}.png', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a51c638d5a1d09f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
